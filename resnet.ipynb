{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a15a05a-e3ed-4c19-8646-fd5f87cde39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4575ba8d-f21b-4730-8844-0741b7488c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spiking Basic Block for ResNet\n",
    "class SpikingBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, beta=0.9, spike_grad=surrogate.fast_sigmoid(slope=25)):\n",
    "        super(SpikingBasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)  # LIF after residual addition\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x if self.downsample is None else self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.lif1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.lif2(out)\n",
    "        out = self.lif3(out + residual)\n",
    "        return out\n",
    "\n",
    "# Modified Spiking ResNet for MNIST\n",
    "class SpikingResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, beta=0.9, spike_grad=surrogate.fast_sigmoid(slope=25), num_steps=25):\n",
    "        super(SpikingResNet, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        # Adjusted for 1-channel 28x28 MNIST input\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)  # Smaller kernel, no stride\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)  # Reduced pooling size\n",
    "\n",
    "        # Smaller channel sizes for MNIST\n",
    "        self.layer1 = self._make_layer(16, 16, blocks=2, stride=1, beta=beta, spike_grad=spike_grad)\n",
    "        self.layer2 = self._make_layer(16, 32, blocks=2, stride=2, beta=beta, spike_grad=spike_grad)\n",
    "        self.layer3 = self._make_layer(32, 64, blocks=2, stride=2, beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64 * SpikingBasicBlock.expansion, num_classes)\n",
    "        self.lif_fc = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride, beta, spike_grad):\n",
    "        layers = []\n",
    "        layers.append(SpikingBasicBlock(in_channels, out_channels, stride, beta, spike_grad))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(SpikingBasicBlock(out_channels, out_channels, beta=beta, spike_grad=spike_grad))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 1, 28, 28)\n",
    "        # Repeat input over time steps: (num_steps, batch, 1, 28, 28)\n",
    "        x = x.unsqueeze(0).repeat(self.num_steps, 1, 1, 1, 1)\n",
    "\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "        for step in range(self.num_steps):\n",
    "            out = self.conv1(x[step])\n",
    "            out = self.lif1(out)\n",
    "            out = self.maxpool(out)\n",
    "\n",
    "            out = self.layer1(out)\n",
    "            out = self.layer2(out)\n",
    "            out = self.layer3(out)\n",
    "\n",
    "            out = self.avgpool(out)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            spk, mem = self.lif_fc(self.fc(out))\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16441066-2404-4b5e-bd3b-93f7671ac55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data loading and preprocessing for MNIST\n",
    "batch_size = 128\n",
    "data_path = './data'\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0,), (1,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8b416f-d18b-4095-9660-88abf667e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    spk_rec, _ = net(data)\n",
    "    acc = SF.accuracy_rate(spk_rec, targets)\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89755dcb-affc-4690-b4d6-350c2558b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network and optimizer\n",
    "num_epochs = 1\n",
    "num_steps = 25\n",
    "net = SpikingResNet(num_classes=10, num_steps=num_steps).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.ce_rate_loss()  # Use ce_count_loss for efficiency with rate coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed514ebb-5f77-488d-a522-8527c2fc4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "spk_rec, mem_rec = net(data)\n",
    "loss_fn(spk_rec, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72028d5-5db2-4936-982b-cf6d40856694",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = loss_fn(spk_rec, targets)\n",
    "optimizer.zero_grad()\n",
    "loss_val.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69cbaffc-04a1-4c4d-9add-7e45c040d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053e8bb-a171-443d-a12b-0520a9ff729d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6c7dbb-69eb-4854-bd57-5081e3f97c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0\n",
      "Train Set Loss: 2.31\n",
      "Test Set Loss: 2.31\n",
      "Train set accuracy for a single minibatch: 5.47%\n",
      "Test set accuracy for a single minibatch: 10.16%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 5\n",
      "Train Set Loss: 2.30\n",
      "Test Set Loss: 2.30\n",
      "Train set accuracy for a single minibatch: 10.94%\n",
      "Test set accuracy for a single minibatch: 7.81%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 10\n",
      "Train Set Loss: 2.30\n",
      "Test Set Loss: 2.30\n",
      "Train set accuracy for a single minibatch: 12.50%\n",
      "Test set accuracy for a single minibatch: 13.28%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 15\n",
      "Train Set Loss: 2.30\n",
      "Test Set Loss: 2.31\n",
      "Train set accuracy for a single minibatch: 10.16%\n",
      "Test set accuracy for a single minibatch: 9.38%\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Gradient calculation and weight update\u001b[39;00m\n\u001b[32m     23\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mloss_val\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m optimizer.step()\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Store loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\snn311\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\snn311\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\snn311\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\snn311\\Lib\\site-packages\\torch\\autograd\\function.py:292\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    288\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    293\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    296\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    297\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "test_acc_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data)  # spk_rec: (num_steps, batch_size, num_classes)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation and weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set evaluation\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test forward pass\n",
    "            test_spk, test_mem = net(test_data)\n",
    "            test_loss = loss_fn(test_spk, test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "            test_acc = SF.accuracy_rate(test_spk, test_targets)\n",
    "            test_acc_hist.append(test_acc)\n",
    "\n",
    "            # Print progress\n",
    "            if counter % 5 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583d2826-7bba-4f93-b4a5-5bb075c55538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3067405223846436,\n",
       " 2.3067405223846436,\n",
       " 2.305178165435791,\n",
       " 2.3025853633880615,\n",
       " 2.3025853633880615,\n",
       " 2.3025853633880615,\n",
       " 2.3025853633880615,\n",
       " 2.305803060531616,\n",
       " 2.3064279556274414,\n",
       " 2.305490493774414,\n",
       " 2.3039281368255615,\n",
       " 2.304865598678589,\n",
       " 2.304865598678589,\n",
       " 2.3045530319213867,\n",
       " 2.3036155700683594,\n",
       " 2.304865598678589]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a68a16-943b-41ad-86de-29c98dcc5209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e276194e-d9c6-44bf-b7bb-532eef3eb68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpikingResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (lif1): Leaky()\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): SpikingBasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif1): Leaky()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif2): Leaky()\n",
      "      (lif3): Leaky()\n",
      "    )\n",
      "    (1): SpikingBasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif1): Leaky()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif2): Leaky()\n",
      "      (lif3): Leaky()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): SpikingBasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (lif1): Leaky()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif2): Leaky()\n",
      "      (lif3): Leaky()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): SpikingBasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif1): Leaky()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif2): Leaky()\n",
      "      (lif3): Leaky()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): SpikingBasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (lif1): Leaky()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif2): Leaky()\n",
      "      (lif3): Leaky()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): SpikingBasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif1): Leaky()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif2): Leaky()\n",
      "      (lif3): Leaky()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): SpikingBasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (lif1): Leaky()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif2): Leaky()\n",
      "      (lif3): Leaky()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): SpikingBasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif1): Leaky()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (lif2): Leaky()\n",
      "      (lif3): Leaky()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (lif_fc): Leaky()\n",
      ")\n",
      "Output spikes shape: torch.Size([25, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "\n",
    "class SpikingBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, beta=0.9, spike_grad=surrogate.fast_sigmoid(slope=25)):\n",
    "        super(SpikingBasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)  # LIF after residual addition\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x if self.downsample is None else self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.lif1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.lif2(out)\n",
    "        out = self.lif3(out + residual)\n",
    "        return out\n",
    "\n",
    "class SpikingResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, beta=0.9, spike_grad=surrogate.fast_sigmoid(slope=25), num_steps=25):\n",
    "        super(SpikingResNet, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 64, blocks=2, stride=1, beta=beta, spike_grad=spike_grad)\n",
    "        self.layer2 = self._make_layer(64, 128, blocks=2, stride=2, beta=beta, spike_grad=spike_grad)\n",
    "        self.layer3 = self._make_layer(128, 256, blocks=2, stride=2, beta=beta, spike_grad=spike_grad)\n",
    "        self.layer4 = self._make_layer(256, 512, blocks=2, stride=2, beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * SpikingBasicBlock.expansion, num_classes)\n",
    "        self.lif_fc = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride, beta, spike_grad):\n",
    "        layers = []\n",
    "        layers.append(SpikingBasicBlock(in_channels, out_channels, stride, beta, spike_grad))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(SpikingBasicBlock(out_channels, out_channels, beta=beta, spike_grad=spike_grad))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, height, width)\n",
    "        # For SNN, repeat input over time steps: (num_steps, batch, channels, height, width)\n",
    "        x = x.unsqueeze(0).repeat(self.num_steps, 1, 1, 1, 1)\n",
    "\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "        for step in range(self.num_steps):\n",
    "            out = self.conv1(x[step])\n",
    "            out = self.lif1(out)\n",
    "            out = self.maxpool(out)\n",
    "\n",
    "            out = self.layer1(out)\n",
    "            out = self.layer2(out)\n",
    "            out = self.layer3(out)\n",
    "            out = self.layer4(out)\n",
    "\n",
    "            out = self.avgpool(out)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            spk, mem = self.lif_fc(self.fc(out))\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    num_steps = 25\n",
    "    model = SpikingResNet(num_classes=10, num_steps=num_steps)\n",
    "    print(model)\n",
    "\n",
    "    # Test with dummy input (batch_size=1, channels=3, 224x224 for ResNet input)\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    spk, mem = model(dummy_input)\n",
    "    print(f\"Output spikes shape: {spk.shape}\")  # (num_steps, batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017576b4-bc66-4e7d-9c9b-e943cc0ee306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snn311)",
   "language": "python",
   "name": "snn311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
