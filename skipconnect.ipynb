{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e56a76-265f-4a8d-adb8-748e8db734a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import functional as SF\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d02d260-2b6b-4af7-a71f-17593f1820ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 128\n",
    "data_path='./data'\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b59960e-1b8c-4d63-8a29-c5ca854349cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.conv1 = nn.Conv2d(1, 12, 5)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.conv2 = nn.Conv2d(12, 64, 5)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.fc1 = nn.Linear(64*4*4, 10)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "        \n",
    "        # record spike and membrane\n",
    "        for step in range(num_steps):\n",
    "            cur1 = F.max_pool2d(self.conv1(x), 2)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            cur3 = self.fc1(spk2.view(batch_size, -1))\n",
    "            spk3, mem3 = self.lif3(cur3)\n",
    "\n",
    "            spk_rec.append(spk3)\n",
    "            mem_rec.append(mem3)\n",
    "            \n",
    "        return torch.stack(spk_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d11707e-84b8-41f7-a1ec-b1b5f426f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, beta, spike_grad, num_steps):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # Initialize layers\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=5, padding=2)  # Output: 12x28x28\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.conv2 = nn.Conv2d(12, 12, kernel_size=5, padding=2)  # Output: 12x14x14\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.fc1 = nn.Linear(12*7*7, 10)  # Adjusted for 12 channels, 7x7 after pooling\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            # Conv1 + pooling\n",
    "            cur1 = self.conv1(x)  # 12x28x28\n",
    "            cur1_pooled = F.max_pool2d(cur1, 2)  # 12x14x14\n",
    "            spk1, mem1 = self.lif1(cur1_pooled, mem1)\n",
    "\n",
    "            # Conv2 + skip connection + pooling\n",
    "            cur2 = self.conv2(spk1)  # 12x14x14\n",
    "            cur2 = cur2 + cur1_pooled  # Skip connection: add conv1 output (12x14x14)\n",
    "            cur2_pooled = F.max_pool2d(cur2, 2)  # 12x7x7\n",
    "            spk2, mem2 = self.lif2(cur2_pooled, mem2)\n",
    "\n",
    "            # Fully connected layer\n",
    "            cur3 = self.fc1(spk2.view(spk2.size(0), -1))  # Dynamic batch size\n",
    "            spk3, mem3 = self.lif3(cur3)\n",
    "\n",
    "            spk_rec.append(spk3)\n",
    "            mem_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fb2f527-d275-424e-833f-e9942c4cdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal SCNN with skip connection\n",
    "class SCNN(nn.Module):\n",
    "    def __init__(self, beta=0.9, spike_grad=None, num_steps=25):\n",
    "        super(SCNN, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        # Conv1: 3x32x32 -> 16x32x32 (padding=1 to maintain size)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        # Conv2: 16x16x16 (after pooling) -> 16x16x16\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        # FC layer: 16x16x16 -> 10\n",
    "        self.fc = nn.Linear(16 * 16 * 16, 10)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize membrane potentials\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            # Conv1 + pooling\n",
    "            cur1 = self.conv1(x)\n",
    "            cur1 = self.bn1(cur1)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            spk1_pooled = F.max_pool2d(spk1, 2)  # 16x16x16\n",
    "\n",
    "            # Conv2 + skip connection (add conv1 output after pooling)\n",
    "            cur2 = self.conv2(spk1_pooled)\n",
    "            cur2 = self.bn2(cur2)\n",
    "            # cur2 = cur2 + spk1_pooled  # Skip connection: same dimensions (16x16x16)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            # FC layer\n",
    "            cur3 = self.fc(spk2.view(spk2.size(0), -1))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk_rec.append(spk3)\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63403a39-bea6-4c5f-b68a-ce5e3b65855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "beta = 0.9\n",
    "spike_grad = None\n",
    "num_steps = 25\n",
    "\n",
    "net = Net(beta, spike_grad, num_steps)\n",
    "# net = SCNN()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_acc_hist = []\n",
    "counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0ec59f3-35c2-4f64-b3d0-111143fbb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "# Data loading for CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "209d27f5-095e-4422-82ea-a708ccf33be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 2.3248\n",
      "iter 1: loss 2.3191\n",
      "iter 2: loss 2.2888\n",
      "iter 3: loss 2.2224\n",
      "iter 4: loss 2.2012\n",
      "iter 5: loss 2.1336\n",
      "iter 6: loss 2.1131\n",
      "iter 7: loss 2.0952\n",
      "iter 8: loss 2.1050\n",
      "iter 9: loss 2.1173\n",
      "iter 10: loss 2.0628\n",
      "iter 11: loss 2.0534\n",
      "iter 12: loss 2.1113\n",
      "iter 13: loss 2.0825\n",
      "iter 14: loss 2.0576\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Gradient calculation + weight update\u001b[39;00m\n\u001b[32m     16\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss_val\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Store loss history for future plotting\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\snn311\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\snn311\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\snn311\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\snn311\\Lib\\site-packages\\torch\\autograd\\function.py:292\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    288\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    293\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    296\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    297\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training loop\n",
    "    for data, targets in iter(train_loader):\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        utils.reset(net)  # resets hidden states for all LIF neurons in net \n",
    "        spk_rec = net(data)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "        print(f'iter {counter}: loss {loss_val.item():.4f}')\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17ad99-807c-4743-96b4-685facb6c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(1, 12, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, init_hidden=True),\n",
    "                    nn.Conv2d(12, 64, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, init_hidden=True),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(64*4*4, 10),\n",
    "                    snn.Leaky(beta=beta, init_hidden=True, output=True)\n",
    "                    )\n",
    "\n",
    "def forward_pass(net, num_steps, data):\n",
    "  mem_rec = []\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "  \n",
    "  # record spike and membrane\n",
    "  for step in range(num_steps):\n",
    "      spk_out, mem_out = net(data)\n",
    "      spk_rec.append(spk_out)\n",
    "      mem_rec.append(mem_out)\n",
    "\n",
    "  return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "cnn_optimizer = torch.optim.Adam(cnn.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_acc_hist = []\n",
    "counter = 0\n",
    "\n",
    "# overall accuracy\n",
    "# def batch_accuracy(train_loader, net, num_steps):\n",
    "#   with torch.no_grad():\n",
    "#     total = 0\n",
    "#     acc = 0\n",
    "#     net.eval()\n",
    "#     train_loader = iter(train_loader)\n",
    "#     for data, targets in train_loader:\n",
    "#       spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "#       acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "#       total += spk_rec.size(1)\n",
    "#   return acc/total\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training loop\n",
    "    for data, targets in iter(train_loader):\n",
    "\n",
    "        # forward pass\n",
    "        cnn.train()\n",
    "        spk_rec, _ = forward_pass(cnn, num_steps, data)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "        print(f'iter {counter}: loss {loss_val.item():.4f}')\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        cnn_optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        cnn_optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        # if counter % 5 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         cnn.eval()\n",
    "\n",
    "        #         # Test set forward pass\n",
    "        #         test_acc = batch_accuracy(test_loader, cnn, num_steps)\n",
    "        #         print(f\"Iteration {counter}, Test Acc: {test_acc * 100:.2f}%\\n\")\n",
    "        #         test_acc_hist.append(test_acc.item())\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74810cfd-9d7e-454c-a471-f614705c707d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snn311)",
   "language": "python",
   "name": "snn311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
